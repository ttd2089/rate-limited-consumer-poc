# Rate-Limited Consumer POC

This is a POC for a rate-limited consumer application.

## The Problem

Consider a hypothetical system where a component `consumer` consumes messages from another component `producer` and makes calls to another component `dependency` while processing the messages. There is a relationship between the rate of messages from `producer` (`r`) and the pressure that `consumer` will put on `dependency` (`p`). Specifically, `p = r * s` where `s`
is the pressure `consumer` puts on `dependency` when it processes a single message.

_NOTE: We don't need to specify units for `p` or worry about variability in `r` over time or in `s` over different control flow cases because each time slice and control flow case simply represents a different solution and our analysis will apply to any solution for the equation._

Suppose there is also a limit `l` on the amount of pressure `dependency` can handle in terms of units over time, e.g. requests per second, before it stops behaving correctly. We can describe the limit as a domain restriction `p <= l`, i.e. `r * s <= l`, for correct and successful operation of our system. We can also describe the restriction in terms of `r`: `r < l / s`, which states that there is a limit `c = l / s` on the rate at which `consumer` can process messages from `producer`.

Next, suppose that the messages being produced by `producer` are the canonical representation of some behviour of the system, e.g. orders that have been paid for, that the consumer implements a necessary next step, e.g. fulfillment of the order, and that `dependency` is required for consumer to do its job, e.g. it's the inventory service that needs to be updated to reflect items taken from the inventory to fulfill the order. Fullfillment (`consumer`) absolutely has to consume and handle _every single_ message from the order `producer`, and it has to keep up with whatever rate orders are produced at. Some amount of transient lag is acceptable, but if the average rate that fulfillment can process orders at is consistently lower than the rate that orders are produced at then the latency for fulfillment of any given order will increase indefinitely -- obviously not acceptable. Since fulfillment (`consumer`) depends on inventory (`dependency`), this means inventory also has to be able to keep up with the rate of order production.

Finally, suppose these systems exist in a distributed system using a microservice architecture and an SRE operational paradigm where different teams are responsible for the different components and each team operates their own services. Every system has operating constraints so we know that `l` and `c` are real values. Unless there is a cap on `r` that ensures `r <= c` then it's possible that consumer will experience failures because `p > l`. Software fails and we have to be ready for that, but this case has some aggrevating factors that can complicate the response and escalate the impact of the degredation if the consumer isn't designed to handle this state gracefully.

In the best case `dependency` will have explicit rate limiting to protect itself from being overwhelmed, `consumer` will honour the rate limit by backing off, and the `consumer` team will have an alert to indicate that the rate limit is activating and lag is accumulating. If `dependency` doesn't have explicit rate limiting then `p > l` instead represents the pressure where the system fails and `consumer` still encounters errors. Ideally in that case both teams are getting alerts, though one team would likely engage the other as part of the response regardless. If neither team is alerted then the issue could go unnoticed until there's business impact. `producer` is likely not monitoring the consumption of the messages it produces so they don't know there's an issue until `consumer` engages them about the volume of messages. This happens because the "correct" response depends on why `r > c`. If `r` has simply grown with usage then `dependency` needs to increase `l` to support the legitimate traffic (or `consumer` needs to get better at smoothing, but this only handles transient peaks in `r`). On the other hand, if `producer` is producing extra messages in error then they need to fix that. In either case, `consumer` cannot operate "correctly" until all parties identify and agree on the problem and someone else delivers a solution. A certain amount of coupling in distributed systems is unavoidable, as is the need for downstream components like `dependency` handle the traffic produced by upstream components like `producer`, but ideally that's something we plan for and deliver in a design/iteration context rather than as incident response.

While we're working on the solution, messages are not being processed correctly. Since the reason is `r > c`, we're accumulating a lag of `t * (r - c)` where `t` represents the duration during which `r > c`. This means the latency will grow over the course of the incident until it's fixed -- messages produced 5 minutes into the incident will take 5x as long to be processed as messages produced 1 minute into the incident. How harmful this is, and how quickly the latency becomes unacceptable, depends on the business context. If the orders are for dog toys that take 3 business days to ship anyway it's probably not the end of the world if the confirmation email takes 5 minutes instead of 30 seconds to show up. If the orders are for something a user expects quickly such as a delivery from a restaurant then 5 minutes with no confirmation could be enough that a user orders elsewhere instead. If the `producer`, `consumer`, `dependency` represents something else entirely, like authz token revocation or security incident alerting, 5 minutes of latency could be catestrophic. We also need to consider the possibility of casscading failures.

## The ~~Solution~~ Mitigation

The solution is obvious: get `p` back down below `l`. This probably means either raising `l` or lowering `r`, or _maybe_ lowering `s`, but these options likely involve multiple teams, decision making, perhaps even creativity, and we don't want to rely on those things during incident response. It's unavoidable that for any given `l` there's some `r` that will result in `p > l` and there's nothing `consumer` can do to prevent or resolve that on its own, but it can be deliberate about what happens _when_ `p > l`.

The problem `consumer` is facing is that it can't keep up with all of the messages, but maybe it can be selective about the messages it does process to make the best use of the capacity is has. The first hurdle here is that message queues and message logs are typically ordered. A naive Kafka consumer for example will consume a message, process it, then commit that it's read that message and move onto the next. If it doesn't commit the offset for the first message it wlil never the second one.

<!-- INSERT DIAGRAM HERE -->

A common solution here is to put failed messages into a 'retry queue' and commit the Kafka offset anyway. Here, commiting still represents the consumer being finished with the Kafka message, but successful handling of the message means _either_ having processed it successfully _or_ having it durably stored elsewhere for later retry. This approach works well for failures that are based on the message itself, i.e. where one message fails but the next one is likely to succeed, because it allows subsequent "good" messages to be processed despite previous "bad" messages. While processing the "bad" messages is deferred, the "good" messages, hopefully the majority, are processed at the regular rate. Our failure mode is different though because it's `dependency` rather than a particular message that's failing. This means any given message is equally likely to fail and the retry queue is just going to steadily grow while `consume` processes an arbitrary subset of the messages from its retry queue and from `consumer` directly up to `l`.

<!-- INSERT DIAGRAM HERE -->

Since we're responding to, or rather trying to preempt, the attempt to process more messages than `dependency` can handle, we don't need to wait for an individual message to fail before we defer it. In fact, if we can identify a message as a good candidate for deferral then we don't want to try it at all since it might work and use up bandwidth we could have otherwise saved for processing more important messages. Instead, we want to inspect messages as they come in and decide whether we should attempt to process them now or defer them until `r` is lower and we have bandwidth to spare. We can put the deferred messages into a queue just like we would with retries, but we should put them in a separate queue so we can distinguish between messages we deferred based on inspection and messages that failed on their own merits. This will be useful for metrics and for allowing us to use different conditions to decide when to consume from each queue.

<!-- INSERT DIAGRAM HERE -->

How we decide which messages to defer depends on the nature of the system and the messages. If the messages are associated with users then maybe there's a subscription level we can associate with message priority. If the messages themselves have a priority/severity/urgency then a simple business rule can be used. In this example, we'll rate limit messages using "customer" and "type" properties so that if a particular customer or message type becomes too noisy then we start to defer it.

<!-- INSERT DIAGRAM HERE -->
